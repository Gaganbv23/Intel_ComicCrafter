# -*- coding: utf-8 -*-
"""ECE_comic_generator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W1PtfQCpOqAyNPh7UEYScEoSolJ6wlLC
"""

# Install required libraries
!pip install -q transformers diffusers torch matplotlib bitsandbytes gradio

# Import libraries
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from diffusers import StableDiffusionPipeline
import torch
import gc
from PIL import Image, ImageDraw, ImageFont
import gradio as gr
import random

# Verify GPU availability
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# Hugging Face token for gated models (replace with your token)
hf_token = "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"

# Configure 4-bit quantization for text generation (only if CUDA is available)
quantization_config = None
if device == "cuda":
    try:
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16
        )
    except Exception as e:
        print(f"Quantization not supported: {e}. Falling back to CPU.")
        device = "cpu"
        quantization_config = None

# Load the text generation model (Mistral 7B)
try:
    text_model_name = "mistralai/Mistral-7B-v0.1"
    text_tokenizer = AutoTokenizer.from_pretrained(text_model_name, use_auth_token=hf_token)
    text_model = AutoModelForCausalLM.from_pretrained(
        text_model_name,
        quantization_config=quantization_config,
        device_map="auto",
        use_auth_token=hf_token
    )
except Exception as e:
    print(f"Failed to load text model: {e}")
    # Fallback to a smaller model
    text_model_name = "gpt2"
    text_tokenizer = AutoTokenizer.from_pretrained(text_model_name)
    text_model = AutoModelForCausalLM.from_pretrained(text_model_name).to(device)

# Load the image generation model (Stable Diffusion)
try:
    image_model_name = "runwayml/stable-diffusion-v1-5"
    image_pipe = StableDiffusionPipeline.from_pretrained(image_model_name, torch_dtype=torch.float16)
    image_pipe = image_pipe.to(device)
except Exception as e:
    print(f"Failed to load image model: {e}")
    raise RuntimeError("Image model could not be loaded. Please check your setup.")

# Function to generate text (story or dialogue)
def generate_text(prompt):
    try:
        with torch.no_grad():
            inputs = text_tokenizer(prompt, return_tensors="pt").to(device)
            outputs = text_model.generate(
                **inputs,
                max_length=300,
                num_return_sequences=1,
                temperature=0.7,
                top_k=50,
                top_p=0.9,
                do_sample=True,
                pad_token_id=text_tokenizer.eos_token_id
            )
            generated_text = text_tokenizer.decode(outputs[0], skip_special_tokens=True)

            # Clean up
            del inputs, outputs
            gc.collect()
            torch.cuda.empty_cache()

            return generated_text
    except Exception as e:
        print(f"Error generating text: {e}")
        return f"Error generating story: {str(e)}"

# Function to generate a retro cartoon-style image
def generate_retro_cartoon_image(prompt):
    try:
        retro_cartoon_prompt = (
            f"{prompt}, classic 1940s cartoon style, hand-drawn animation, "
            "bold outlines, vibrant colors, cel-shaded, exaggerated expressions, "
            "vintage animation, retro cartoon"
        )
        with torch.no_grad():
            image = image_pipe(
                retro_cartoon_prompt,
                height=512,
                width=512,
                num_inference_steps=30  # More steps for better quality
            ).images[0]
        return image
    except Exception as e:
        print(f"Error generating image: {e}")
        # Create error placeholder image
        img = Image.new('RGB', (512, 512), color=(255, 200, 200))
        draw = ImageDraw.Draw(img)
        draw.text((50, 250), "Failed to generate image", fill=(0, 0, 0))
        return img

# Function to split story into 4 parts for panels
def split_story(story):
    sentences = [s.strip() for s in story.split('.') if s.strip()]
    if len(sentences) < 4:
        # If not enough sentences, duplicate some
        sentences = sentences * (4 // len(sentences) + 1)

    # Distribute sentences across 4 panels
    panel_texts = []
    for i in range(4):
        start = i * len(sentences) // 4
        end = (i + 1) * len(sentences) // 4
        panel_text = '. '.join(sentences[start:end]) + '.'
        panel_texts.append(panel_text)

    return panel_texts

# Function to create comic panel with text
def create_comic_panel(text, image, panel_size=(512, 512)):
    try:
        panel = Image.new("RGB", panel_size, "white")
        draw = ImageDraw.Draw(panel)

        # Resize and paste image (top 75% of panel)
        img_height = int(panel_size[1] * 0.75)
        panel.paste(image.resize((panel_size[0], img_height)), (0, 0))

        # Add text (bottom 25%)
        font = ImageFont.load_default()
        text_position = (10, img_height + 10)
        draw.text(text_position, text, fill="black", font=font)

        return panel
    except Exception as e:
        print(f"Error creating panel: {e}")
        error_img = Image.new('RGB', panel_size, color=(255, 200, 200))
        draw = ImageDraw.Draw(error_img)
        draw.text((50, 250), "Panel creation error", fill=(0, 0, 0))
        return error_img

# Main generation function
def generate_comic(story_prompt):
    try:
        # Generate story
        story = generate_text(f"Write a short story about: {story_prompt}")

        # Split into 4 parts
        panel_texts = split_story(story)

        # Generate panels
        panels = []
        for i, text in enumerate(panel_texts):
            image = generate_retro_cartoon_image(text)
            panel = create_comic_panel(text, image)
            panels.append(panel)

        return story, *panels

    except Exception as e:
        print(f"Error in comic generation: {e}")
        error_img = Image.new('RGB', (512, 512), color=(255, 200, 200))
        draw = ImageDraw.Draw(error_img)
        draw.text((50, 250), "Generation error", fill=(0, 0, 0))
        return f"Error: {str(e)}", error_img, error_img, error_img, error_img

# Custom CSS for styling
css = """
.gradio-container {
    max-width: 1200px !important;
}
.panel-container {
    display: flex;
    flex-wrap: wrap;
    justify-content: center;
    gap: 10px;
    margin-top: 20px;
}
.panel {
    border: 2px solid #ddd;
    border-radius: 8px;
    box-shadow: 0 4px 6px rgba(0,0,0,0.1);
}
.story-box {
    background: #f8f9fa;
    padding: 15px;
    border-radius: 8px;
    margin-bottom: 20px;
}
"""

# Gradio interface
with gr.Blocks(css=css, title="AI Comic Generator") as demo:
    gr.Markdown("""
    # ðŸŽ¨ AI Comic Generator
    Enter a story idea below and the AI will generate a 4-panel comic strip!
    """)

    with gr.Row():
        with gr.Column():
            story_prompt = gr.Textbox(
                label="Story Prompt",
                placeholder="e.g., 'A robot who wants to be a chef'",
                lines=3
            )
            generate_btn = gr.Button("Generate Comic", variant="primary")

    with gr.Row():
        story_output = gr.Textbox(
            label="Generated Story",
            interactive=False,
            elem_classes=["story-box"]
        )

    with gr.Row(elem_classes=["panel-container"]):
        panel_outputs = []
        for i in range(4):
            panel_outputs.append(
                gr.Image(
                    label=f"Panel {i+1}",
                    elem_classes=["panel"],
                    width=512,
                    height=512
                )
            )

    generate_btn.click(
        fn=generate_comic,
        inputs=story_prompt,
        outputs=[story_output] + panel_outputs
    )

# Launch the app
demo.launch(share=True)